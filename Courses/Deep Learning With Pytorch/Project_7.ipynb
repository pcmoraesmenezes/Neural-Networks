{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 7\n",
        "\n",
        "Grid Search + Parameter Tuning + DropOut in Iris dataset"
      ],
      "metadata": {
        "id": "3w6cEQE8JgA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGaKN-sVK5hs",
        "outputId": "5b743b71-633d-4006-8472-1578f4d9ef34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: skorch in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-GOG0ve2JW1d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from skorch import NeuralNetBinaryClassifier\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from skorch import NeuralNetClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/iris.csv')"
      ],
      "metadata": {
        "id": "lj59isp4K_9U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfNbCRK5LB1n",
        "outputId": "9e62ec5c-fe99-40a1-a6b9-4fb0a394be59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f611b3a3e50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forecasters = df.iloc[:, 0:4].values\n",
        "labels = df.iloc[:, 4].values"
      ],
      "metadata": {
        "id": "5pht-E1RLSmF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "labels = encoder.fit_transform(labels)"
      ],
      "metadata": {
        "id": "025qN0-jLf0_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecasters = np.array(forecasters, dtype='float32')\n",
        "labels = labels.astype('int64')"
      ],
      "metadata": {
        "id": "tAjL6-byNuh5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class torch_classifier(nn.Module):\n",
        "  def __init__(self, activation, neurons, initializer, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense0 = nn.Linear(4, neurons)\n",
        "    initializer(self.dense0.weight)\n",
        "    self.activation0 = activation\n",
        "\n",
        "    self.dense1 = nn.Linear(neurons, neurons)\n",
        "    initializer(self.dense1.weight)\n",
        "    self.activation1 = activation\n",
        "    self.dense2 = nn.Linear(neurons, 3)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.dense0(X)\n",
        "    X = self.activation0(X)\n",
        "    X = self.dropout(X)\n",
        "\n",
        "    X = self.dense1(X)\n",
        "    X = self.activation1(X)\n",
        "    X = self.dropout(X)\n",
        "\n",
        "    X = self.dense2(X)\n",
        "\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "mpC_et0VUTI7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_classifier = NeuralNetClassifier(module=torch_classifier,\n",
        "                                         criterion = torch.nn.CrossEntropyLoss,\n",
        "                                         train_split=False)"
      ],
      "metadata": {
        "id": "qYJkHegyU0nl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'batch_size': [10],\n",
        "          'max_epochs': [100],\n",
        "          'optimizer': [torch.optim.Adam],\n",
        "          'module__activation': [F.relu],\n",
        "          'module__neurons': [4, 8],\n",
        "          'module__initializer': [torch.nn.init.uniform_],\n",
        "          'module__dropout': [0.2, 0.3]\n",
        "          }"
      ],
      "metadata": {
        "id": "0P4Q_xqGU_2B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(estimator = sklearn_classifier,\n",
        "                           param_grid = params,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 3)"
      ],
      "metadata": {
        "id": "nS0qx0zfVBX4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = grid_search.fit(forecasters, labels);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enewXYBpVEBW",
        "outputId": "9415fd91-2431-4837-bd50-75f95bc368ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m10.5902\u001b[0m  0.0174\n",
            "      2        \u001b[36m5.1621\u001b[0m  0.0185\n",
            "      3        \u001b[36m2.8981\u001b[0m  0.0166\n",
            "      4        \u001b[36m2.1835\u001b[0m  0.0183\n",
            "      5        \u001b[36m1.7394\u001b[0m  0.0176\n",
            "      6        1.8199  0.0164\n",
            "      7        \u001b[36m1.5159\u001b[0m  0.0184\n",
            "      8        \u001b[36m1.2023\u001b[0m  0.0167\n",
            "      9        1.4016  0.0174\n",
            "     10        1.2750  0.0166\n",
            "     11        \u001b[36m1.0696\u001b[0m  0.0242\n",
            "     12        1.1939  0.0209\n",
            "     13        1.1130  0.0162\n",
            "     14        \u001b[36m1.0233\u001b[0m  0.0174\n",
            "     15        1.0344  0.0192\n",
            "     16        \u001b[36m0.9520\u001b[0m  0.0171\n",
            "     17        0.9782  0.0178\n",
            "     18        0.9875  0.0200\n",
            "     19        0.9993  0.0168\n",
            "     20        \u001b[36m0.9127\u001b[0m  0.0165\n",
            "     21        \u001b[36m0.9126\u001b[0m  0.0231\n",
            "     22        0.9506  0.0170\n",
            "     23        0.9395  0.0160\n",
            "     24        \u001b[36m0.8545\u001b[0m  0.0172\n",
            "     25        0.8596  0.0170\n",
            "     26        \u001b[36m0.8302\u001b[0m  0.0173\n",
            "     27        \u001b[36m0.8161\u001b[0m  0.0172\n",
            "     28        0.8173  0.0167\n",
            "     29        \u001b[36m0.7735\u001b[0m  0.0164\n",
            "     30        0.8056  0.0190\n",
            "     31        \u001b[36m0.7376\u001b[0m  0.0176\n",
            "     32        0.7854  0.0175\n",
            "     33        \u001b[36m0.7203\u001b[0m  0.0179\n",
            "     34        0.7505  0.0183\n",
            "     35        0.7383  0.0169\n",
            "     36        \u001b[36m0.6871\u001b[0m  0.0168\n",
            "     37        \u001b[36m0.6851\u001b[0m  0.0167\n",
            "     38        0.6938  0.0177\n",
            "     39        \u001b[36m0.6620\u001b[0m  0.0202\n",
            "     40        0.6807  0.0265\n",
            "     41        0.6929  0.0198\n",
            "     42        \u001b[36m0.6409\u001b[0m  0.0169\n",
            "     43        0.6447  0.0170\n",
            "     44        0.6739  0.0182\n",
            "     45        0.6806  0.0187\n",
            "     46        \u001b[36m0.6402\u001b[0m  0.0212\n",
            "     47        \u001b[36m0.6316\u001b[0m  0.0208\n",
            "     48        \u001b[36m0.6056\u001b[0m  0.0177\n",
            "     49        \u001b[36m0.5816\u001b[0m  0.0173\n",
            "     50        0.6117  0.0182\n",
            "     51        0.5890  0.0169\n",
            "     52        0.6419  0.0171\n",
            "     53        \u001b[36m0.5656\u001b[0m  0.0172\n",
            "     54        \u001b[36m0.5475\u001b[0m  0.0179\n",
            "     55        0.6029  0.0170\n",
            "     56        0.5702  0.0178\n",
            "     57        0.5752  0.0178\n",
            "     58        0.5784  0.0168\n",
            "     59        0.5766  0.0169\n",
            "     60        0.5653  0.0173\n",
            "     61        \u001b[36m0.5464\u001b[0m  0.0173\n",
            "     62        0.5712  0.0173\n",
            "     63        \u001b[36m0.5280\u001b[0m  0.0187\n",
            "     64        \u001b[36m0.5193\u001b[0m  0.0162\n",
            "     65        0.5342  0.0171\n",
            "     66        0.5434  0.0171\n",
            "     67        0.5428  0.0173\n",
            "     68        0.5508  0.0167\n",
            "     69        0.5260  0.0170\n",
            "     70        0.5244  0.0160\n",
            "     71        \u001b[36m0.4698\u001b[0m  0.0177\n",
            "     72        0.5615  0.0166\n",
            "     73        0.5029  0.0181\n",
            "     74        0.4939  0.0155\n",
            "     75        0.4928  0.0187\n",
            "     76        0.5816  0.0166\n",
            "     77        0.5132  0.0168\n",
            "     78        \u001b[36m0.4629\u001b[0m  0.0157\n",
            "     79        0.4665  0.0158\n",
            "     80        0.5371  0.0195\n",
            "     81        0.5257  0.0167\n",
            "     82        0.4892  0.0182\n",
            "     83        0.5332  0.0204\n",
            "     84        0.4639  0.0182\n",
            "     85        0.5026  0.0171\n",
            "     86        0.5257  0.0168\n",
            "     87        0.5382  0.0168\n",
            "     88        0.5111  0.0162\n",
            "     89        0.4886  0.0185\n",
            "     90        0.4924  0.0171\n",
            "     91        0.5199  0.0174\n",
            "     92        \u001b[36m0.4585\u001b[0m  0.0168\n",
            "     93        0.4695  0.0253\n",
            "     94        0.4706  0.0151\n",
            "     95        0.5250  0.0171\n",
            "     96        0.4587  0.0181\n",
            "     97        0.4643  0.0194\n",
            "     98        0.4854  0.0150\n",
            "     99        0.4842  0.0168\n",
            "    100        0.4791  0.0164\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m17.2796\u001b[0m  0.0152\n",
            "      2        \u001b[36m9.7857\u001b[0m  0.0175\n",
            "      3        \u001b[36m4.1927\u001b[0m  0.0172\n",
            "      4        \u001b[36m2.9725\u001b[0m  0.0191\n",
            "      5        \u001b[36m2.0379\u001b[0m  0.0159\n",
            "      6        \u001b[36m1.4752\u001b[0m  0.0174\n",
            "      7        \u001b[36m1.3681\u001b[0m  0.0178\n",
            "      8        \u001b[36m1.3422\u001b[0m  0.0176\n",
            "      9        1.3562  0.0168\n",
            "     10        \u001b[36m1.2577\u001b[0m  0.0195\n",
            "     11        1.3055  0.0169\n",
            "     12        1.2601  0.0196\n",
            "     13        1.3052  0.0175\n",
            "     14        \u001b[36m1.1490\u001b[0m  0.0164\n",
            "     15        1.1926  0.0181\n",
            "     16        1.2202  0.0183\n",
            "     17        1.1823  0.0176\n",
            "     18        1.1933  0.0168\n",
            "     19        \u001b[36m1.1301\u001b[0m  0.0214\n",
            "     20        1.1980  0.0175\n",
            "     21        1.1686  0.0292\n",
            "     22        1.1738  0.0230\n",
            "     23        1.1601  0.0280\n",
            "     24        1.1646  0.0222\n",
            "     25        1.1375  0.0233\n",
            "     26        \u001b[36m1.1096\u001b[0m  0.0250\n",
            "     27        1.1132  0.0258\n",
            "     28        \u001b[36m1.0714\u001b[0m  0.0201\n",
            "     29        1.0740  0.0214\n",
            "     30        1.0797  0.0216\n",
            "     31        1.1136  0.0237\n",
            "     32        1.0887  0.0220\n",
            "     33        \u001b[36m1.0657\u001b[0m  0.0216\n",
            "     34        \u001b[36m1.0471\u001b[0m  0.0204\n",
            "     35        \u001b[36m1.0306\u001b[0m  0.0246\n",
            "     36        \u001b[36m0.9903\u001b[0m  0.0208\n",
            "     37        1.0196  0.0210\n",
            "     38        \u001b[36m0.9775\u001b[0m  0.0208\n",
            "     39        \u001b[36m0.9659\u001b[0m  0.0257\n",
            "     40        \u001b[36m0.9257\u001b[0m  0.0239\n",
            "     41        0.9327  0.0253\n",
            "     42        \u001b[36m0.9029\u001b[0m  0.0228\n",
            "     43        \u001b[36m0.8906\u001b[0m  0.0203\n",
            "     44        0.8918  0.0195\n",
            "     45        0.9036  0.0218\n",
            "     46        \u001b[36m0.8796\u001b[0m  0.0192\n",
            "     47        0.8798  0.0210\n",
            "     48        \u001b[36m0.8546\u001b[0m  0.0243\n",
            "     49        \u001b[36m0.8078\u001b[0m  0.0197\n",
            "     50        \u001b[36m0.7928\u001b[0m  0.0215\n",
            "     51        0.8223  0.0204\n",
            "     52        0.7985  0.0202\n",
            "     53        0.8270  0.0201\n",
            "     54        0.8133  0.0210\n",
            "     55        0.8321  0.0205\n",
            "     56        \u001b[36m0.7912\u001b[0m  0.0198\n",
            "     57        0.8190  0.0204\n",
            "     58        \u001b[36m0.7772\u001b[0m  0.0200\n",
            "     59        0.7860  0.0218\n",
            "     60        0.8177  0.0200\n",
            "     61        \u001b[36m0.7634\u001b[0m  0.0191\n",
            "     62        0.7855  0.0184\n",
            "     63        \u001b[36m0.7384\u001b[0m  0.0195\n",
            "     64        0.7723  0.0229\n",
            "     65        0.7722  0.0194\n",
            "     66        0.7580  0.0217\n",
            "     67        \u001b[36m0.7260\u001b[0m  0.0202\n",
            "     68        \u001b[36m0.7145\u001b[0m  0.0202\n",
            "     69        \u001b[36m0.6707\u001b[0m  0.0202\n",
            "     70        0.7087  0.0208\n",
            "     71        0.7331  0.0202\n",
            "     72        0.6779  0.0224\n",
            "     73        \u001b[36m0.6596\u001b[0m  0.0193\n",
            "     74        0.6756  0.0199\n",
            "     75        0.7052  0.0196\n",
            "     76        0.6764  0.0201\n",
            "     77        0.6672  0.0204\n",
            "     78        0.6955  0.0202\n",
            "     79        0.6681  0.0199\n",
            "     80        0.6808  0.0192\n",
            "     81        0.6628  0.0202\n",
            "     82        \u001b[36m0.6440\u001b[0m  0.0200\n",
            "     83        0.6647  0.0200\n",
            "     84        0.6450  0.0250\n",
            "     85        0.6478  0.0294\n",
            "     86        \u001b[36m0.6059\u001b[0m  0.0275\n",
            "     87        0.6766  0.0216\n",
            "     88        \u001b[36m0.6049\u001b[0m  0.0281\n",
            "     89        0.6242  0.0236\n",
            "     90        0.6342  0.0215\n",
            "     91        \u001b[36m0.6014\u001b[0m  0.0240\n",
            "     92        0.6130  0.0251\n",
            "     93        \u001b[36m0.5889\u001b[0m  0.0221\n",
            "     94        \u001b[36m0.5843\u001b[0m  0.0274\n",
            "     95        0.6251  0.0240\n",
            "     96        0.5959  0.0211\n",
            "     97        0.6065  0.0249\n",
            "     98        \u001b[36m0.5755\u001b[0m  0.0225\n",
            "     99        0.6179  0.0250\n",
            "    100        \u001b[36m0.5668\u001b[0m  0.0221\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m8.5766\u001b[0m  0.0245\n",
            "      2        \u001b[36m3.1712\u001b[0m  0.0235\n",
            "      3        \u001b[36m2.5382\u001b[0m  0.0217\n",
            "      4        \u001b[36m1.4819\u001b[0m  0.0197\n",
            "      5        \u001b[36m1.4164\u001b[0m  0.0214\n",
            "      6        1.6618  0.0234\n",
            "      7        \u001b[36m1.3244\u001b[0m  0.0235\n",
            "      8        \u001b[36m1.1540\u001b[0m  0.0255\n",
            "      9        1.2026  0.0233\n",
            "     10        \u001b[36m1.1524\u001b[0m  0.0244\n",
            "     11        \u001b[36m1.0699\u001b[0m  0.0228\n",
            "     12        \u001b[36m0.9777\u001b[0m  0.0235\n",
            "     13        \u001b[36m0.9655\u001b[0m  0.0232\n",
            "     14        1.0057  0.0234\n",
            "     15        \u001b[36m0.9283\u001b[0m  0.0237\n",
            "     16        0.9866  0.0231\n",
            "     17        0.9546  0.0237\n",
            "     18        \u001b[36m0.9007\u001b[0m  0.0261\n",
            "     19        0.9093  0.0161\n",
            "     20        \u001b[36m0.8582\u001b[0m  0.0218\n",
            "     21        \u001b[36m0.8369\u001b[0m  0.0166\n",
            "     22        0.8392  0.0171\n",
            "     23        \u001b[36m0.7936\u001b[0m  0.0170\n",
            "     24        0.8251  0.0164\n",
            "     25        0.8339  0.0246\n",
            "     26        \u001b[36m0.7500\u001b[0m  0.0168\n",
            "     27        0.7578  0.0162\n",
            "     28        \u001b[36m0.7240\u001b[0m  0.0170\n",
            "     29        0.7315  0.0166\n",
            "     30        \u001b[36m0.6750\u001b[0m  0.0169\n",
            "     31        0.6910  0.0169\n",
            "     32        \u001b[36m0.6594\u001b[0m  0.0171\n",
            "     33        0.6688  0.0171\n",
            "     34        0.6621  0.0191\n",
            "     35        \u001b[36m0.6167\u001b[0m  0.0159\n",
            "     36        0.6460  0.0171\n",
            "     37        0.6387  0.0183\n",
            "     38        0.6319  0.0180\n",
            "     39        0.6523  0.0179\n",
            "     40        0.6642  0.0177\n",
            "     41        0.6287  0.0225\n",
            "     42        \u001b[36m0.5969\u001b[0m  0.0174\n",
            "     43        0.6645  0.0188\n",
            "     44        0.6136  0.0167\n",
            "     45        0.6307  0.0174\n",
            "     46        0.6241  0.0160\n",
            "     47        0.6396  0.0174\n",
            "     48        \u001b[36m0.5236\u001b[0m  0.0187\n",
            "     49        0.5795  0.0169\n",
            "     50        0.5646  0.0189\n",
            "     51        0.5462  0.0174\n",
            "     52        0.5533  0.0170\n",
            "     53        0.6369  0.0176\n",
            "     54        0.6385  0.0173\n",
            "     55        0.5781  0.0153\n",
            "     56        0.5258  0.0171\n",
            "     57        \u001b[36m0.5073\u001b[0m  0.0201\n",
            "     58        0.5243  0.0197\n",
            "     59        0.5162  0.0184\n",
            "     60        0.5662  0.0200\n",
            "     61        0.5342  0.0191\n",
            "     62        0.5078  0.0200\n",
            "     63        0.5442  0.0203\n",
            "     64        0.5622  0.0226\n",
            "     65        0.5446  0.0174\n",
            "     66        0.5456  0.0211\n",
            "     67        \u001b[36m0.5048\u001b[0m  0.0179\n",
            "     68        0.5085  0.0182\n",
            "     69        0.5453  0.0181\n",
            "     70        0.5877  0.0184\n",
            "     71        \u001b[36m0.4881\u001b[0m  0.0174\n",
            "     72        0.5099  0.0190\n",
            "     73        0.5584  0.0183\n",
            "     74        0.5422  0.0183\n",
            "     75        0.5134  0.0170\n",
            "     76        \u001b[36m0.4695\u001b[0m  0.0198\n",
            "     77        0.5571  0.0279\n",
            "     78        0.5501  0.0182\n",
            "     79        0.5271  0.0180\n",
            "     80        0.4995  0.0181\n",
            "     81        0.4979  0.0207\n",
            "     82        0.5009  0.0188\n",
            "     83        0.4937  0.0185\n",
            "     84        \u001b[36m0.4551\u001b[0m  0.0183\n",
            "     85        0.4751  0.0201\n",
            "     86        0.4905  0.0190\n",
            "     87        0.4781  0.0190\n",
            "     88        0.4785  0.0187\n",
            "     89        0.5201  0.0217\n",
            "     90        0.5204  0.0192\n",
            "     91        0.5200  0.0191\n",
            "     92        \u001b[36m0.4333\u001b[0m  0.0204\n",
            "     93        0.5044  0.0184\n",
            "     94        0.4611  0.0183\n",
            "     95        0.4615  0.0186\n",
            "     96        0.4840  0.0181\n",
            "     97        0.5014  0.0191\n",
            "     98        0.4757  0.0183\n",
            "     99        \u001b[36m0.4000\u001b[0m  0.0247\n",
            "    100        0.4730  0.0187\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m19.5468\u001b[0m  0.0166\n",
            "      2        \u001b[36m3.3688\u001b[0m  0.0182\n",
            "      3        5.2026  0.0188\n",
            "      4        4.0251  0.0185\n",
            "      5        \u001b[36m2.8802\u001b[0m  0.0186\n",
            "      6        \u001b[36m1.6766\u001b[0m  0.0175\n",
            "      7        \u001b[36m1.5044\u001b[0m  0.0216\n",
            "      8        \u001b[36m1.3882\u001b[0m  0.0181\n",
            "      9        \u001b[36m1.1607\u001b[0m  0.0202\n",
            "     10        1.3157  0.0178\n",
            "     11        1.1691  0.0191\n",
            "     12        \u001b[36m1.1326\u001b[0m  0.0192\n",
            "     13        \u001b[36m1.0464\u001b[0m  0.0181\n",
            "     14        \u001b[36m0.9905\u001b[0m  0.0201\n",
            "     15        \u001b[36m0.9687\u001b[0m  0.0210\n",
            "     16        \u001b[36m0.8994\u001b[0m  0.0187\n",
            "     17        0.9157  0.0178\n",
            "     18        0.9250  0.0195\n",
            "     19        \u001b[36m0.8896\u001b[0m  0.0188\n",
            "     20        0.9459  0.0175\n",
            "     21        \u001b[36m0.8832\u001b[0m  0.0176\n",
            "     22        \u001b[36m0.8217\u001b[0m  0.0177\n",
            "     23        0.8236  0.0168\n",
            "     24        \u001b[36m0.7864\u001b[0m  0.0179\n",
            "     25        0.8325  0.0185\n",
            "     26        \u001b[36m0.7814\u001b[0m  0.0216\n",
            "     27        \u001b[36m0.7721\u001b[0m  0.0243\n",
            "     28        0.8128  0.0172\n",
            "     29        \u001b[36m0.7085\u001b[0m  0.0182\n",
            "     30        0.7757  0.0200\n",
            "     31        0.7759  0.0173\n",
            "     32        0.7762  0.0232\n",
            "     33        \u001b[36m0.7011\u001b[0m  0.0171\n",
            "     34        \u001b[36m0.6415\u001b[0m  0.0187\n",
            "     35        0.6780  0.0208\n",
            "     36        0.6956  0.0192\n",
            "     37        0.6650  0.0184\n",
            "     38        0.7354  0.0179\n",
            "     39        \u001b[36m0.6410\u001b[0m  0.0200\n",
            "     40        0.6444  0.0202\n",
            "     41        \u001b[36m0.6299\u001b[0m  0.0188\n",
            "     42        0.6301  0.0189\n",
            "     43        \u001b[36m0.5942\u001b[0m  0.0191\n",
            "     44        \u001b[36m0.5893\u001b[0m  0.0190\n",
            "     45        0.6313  0.0188\n",
            "     46        0.6402  0.0176\n",
            "     47        0.6307  0.0198\n",
            "     48        \u001b[36m0.5473\u001b[0m  0.0172\n",
            "     49        0.5898  0.0171\n",
            "     50        0.5786  0.0174\n",
            "     51        \u001b[36m0.5464\u001b[0m  0.0223\n",
            "     52        \u001b[36m0.5331\u001b[0m  0.0169\n",
            "     53        0.5763  0.0192\n",
            "     54        \u001b[36m0.5212\u001b[0m  0.0175\n",
            "     55        0.5235  0.0201\n",
            "     56        0.5444  0.0180\n",
            "     57        \u001b[36m0.5199\u001b[0m  0.0168\n",
            "     58        0.5387  0.0172\n",
            "     59        \u001b[36m0.5150\u001b[0m  0.0179\n",
            "     60        0.5431  0.0185\n",
            "     61        \u001b[36m0.4787\u001b[0m  0.0175\n",
            "     62        \u001b[36m0.4445\u001b[0m  0.0161\n",
            "     63        0.4858  0.0172\n",
            "     64        0.4577  0.0167\n",
            "     65        0.4886  0.0178\n",
            "     66        0.5511  0.0177\n",
            "     67        0.4506  0.0179\n",
            "     68        0.5317  0.0170\n",
            "     69        \u001b[36m0.4371\u001b[0m  0.0185\n",
            "     70        0.4579  0.0174\n",
            "     71        0.4646  0.0183\n",
            "     72        \u001b[36m0.4266\u001b[0m  0.0181\n",
            "     73        0.4556  0.0168\n",
            "     74        0.4370  0.0198\n",
            "     75        0.4311  0.0172\n",
            "     76        0.4694  0.0187\n",
            "     77        \u001b[36m0.3828\u001b[0m  0.0166\n",
            "     78        0.4825  0.0213\n",
            "     79        0.4536  0.0223\n",
            "     80        0.4339  0.0218\n",
            "     81        0.4434  0.0174\n",
            "     82        0.4012  0.0169\n",
            "     83        0.4267  0.0198\n",
            "     84        0.4286  0.0180\n",
            "     85        \u001b[36m0.3812\u001b[0m  0.0177\n",
            "     86        0.4497  0.0175\n",
            "     87        \u001b[36m0.3406\u001b[0m  0.0189\n",
            "     88        0.3889  0.0186\n",
            "     89        0.3819  0.0231\n",
            "     90        0.3973  0.0191\n",
            "     91        0.3428  0.0196\n",
            "     92        0.4255  0.0184\n",
            "     93        0.3608  0.0172\n",
            "     94        0.4140  0.0228\n",
            "     95        0.3429  0.0172\n",
            "     96        0.3998  0.0170\n",
            "     97        0.3724  0.0178\n",
            "     98        \u001b[36m0.3022\u001b[0m  0.0194\n",
            "     99        0.3770  0.0173\n",
            "    100        0.3304  0.0188\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m27.0759\u001b[0m  0.0150\n",
            "      2        \u001b[36m7.5350\u001b[0m  0.0169\n",
            "      3        \u001b[36m3.9915\u001b[0m  0.0189\n",
            "      4        \u001b[36m3.8630\u001b[0m  0.0170\n",
            "      5        \u001b[36m3.3184\u001b[0m  0.0174\n",
            "      6        \u001b[36m2.3480\u001b[0m  0.0171\n",
            "      7        \u001b[36m2.3291\u001b[0m  0.0221\n",
            "      8        \u001b[36m2.1770\u001b[0m  0.0169\n",
            "      9        \u001b[36m1.9008\u001b[0m  0.0165\n",
            "     10        \u001b[36m1.2237\u001b[0m  0.0179\n",
            "     11        1.4403  0.0177\n",
            "     12        1.2874  0.0207\n",
            "     13        1.2415  0.0180\n",
            "     14        \u001b[36m1.1932\u001b[0m  0.0204\n",
            "     15        \u001b[36m1.1580\u001b[0m  0.0186\n",
            "     16        1.2018  0.0171\n",
            "     17        1.1591  0.0191\n",
            "     18        \u001b[36m1.1155\u001b[0m  0.0173\n",
            "     19        \u001b[36m1.0931\u001b[0m  0.0165\n",
            "     20        1.1721  0.0181\n",
            "     21        \u001b[36m1.0168\u001b[0m  0.0178\n",
            "     22        \u001b[36m0.9729\u001b[0m  0.0179\n",
            "     23        0.9855  0.0200\n",
            "     24        \u001b[36m0.8831\u001b[0m  0.0246\n",
            "     25        1.0137  0.0233\n",
            "     26        0.9420  0.0202\n",
            "     27        0.9400  0.0195\n",
            "     28        0.9144  0.0258\n",
            "     29        \u001b[36m0.8587\u001b[0m  0.0255\n",
            "     30        0.8662  0.0163\n",
            "     31        0.8815  0.0178\n",
            "     32        \u001b[36m0.8165\u001b[0m  0.0171\n",
            "     33        0.8409  0.0171\n",
            "     34        0.8191  0.0210\n",
            "     35        0.8653  0.0192\n",
            "     36        0.8325  0.0170\n",
            "     37        \u001b[36m0.7719\u001b[0m  0.0173\n",
            "     38        0.8761  0.0169\n",
            "     39        \u001b[36m0.7697\u001b[0m  0.0174\n",
            "     40        \u001b[36m0.7498\u001b[0m  0.0182\n",
            "     41        \u001b[36m0.7482\u001b[0m  0.0174\n",
            "     42        0.7590  0.0168\n",
            "     43        0.7666  0.0185\n",
            "     44        \u001b[36m0.7268\u001b[0m  0.0184\n",
            "     45        \u001b[36m0.7111\u001b[0m  0.0191\n",
            "     46        0.7311  0.0173\n",
            "     47        0.7240  0.0166\n",
            "     48        0.7517  0.0196\n",
            "     49        \u001b[36m0.6688\u001b[0m  0.0176\n",
            "     50        \u001b[36m0.6110\u001b[0m  0.0169\n",
            "     51        0.6242  0.0172\n",
            "     52        0.6962  0.0194\n",
            "     53        0.6395  0.0175\n",
            "     54        0.6614  0.0171\n",
            "     55        0.6279  0.0178\n",
            "     56        0.6794  0.0165\n",
            "     57        0.6146  0.0202\n",
            "     58        0.7081  0.0171\n",
            "     59        0.6354  0.0182\n",
            "     60        0.6303  0.0164\n",
            "     61        0.6247  0.0184\n",
            "     62        \u001b[36m0.5828\u001b[0m  0.0179\n",
            "     63        \u001b[36m0.5469\u001b[0m  0.0185\n",
            "     64        0.5979  0.0180\n",
            "     65        0.5973  0.0183\n",
            "     66        0.6071  0.0211\n",
            "     67        0.5880  0.0175\n",
            "     68        \u001b[36m0.5449\u001b[0m  0.0178\n",
            "     69        0.5820  0.0172\n",
            "     70        0.6338  0.0200\n",
            "     71        0.5732  0.0175\n",
            "     72        0.6138  0.0182\n",
            "     73        \u001b[36m0.5447\u001b[0m  0.0162\n",
            "     74        0.5691  0.0185\n",
            "     75        0.5525  0.0173\n",
            "     76        \u001b[36m0.5270\u001b[0m  0.0193\n",
            "     77        0.5898  0.0175\n",
            "     78        0.5408  0.0181\n",
            "     79        \u001b[36m0.5119\u001b[0m  0.0176\n",
            "     80        0.5613  0.0184\n",
            "     81        \u001b[36m0.4964\u001b[0m  0.0208\n",
            "     82        0.5150  0.0228\n",
            "     83        0.5514  0.0169\n",
            "     84        0.6062  0.0173\n",
            "     85        0.5390  0.0168\n",
            "     86        0.5405  0.0182\n",
            "     87        0.5368  0.0184\n",
            "     88        0.5140  0.0174\n",
            "     89        0.5396  0.0165\n",
            "     90        \u001b[36m0.4361\u001b[0m  0.0185\n",
            "     91        0.5339  0.0176\n",
            "     92        0.5511  0.0192\n",
            "     93        0.4674  0.0172\n",
            "     94        0.4438  0.0163\n",
            "     95        0.5382  0.0193\n",
            "     96        0.5116  0.0172\n",
            "     97        0.4400  0.0176\n",
            "     98        \u001b[36m0.4311\u001b[0m  0.0170\n",
            "     99        0.5403  0.0202\n",
            "    100        0.4855  0.0184\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m13.3825\u001b[0m  0.0147\n",
            "      2        \u001b[36m3.5022\u001b[0m  0.0187\n",
            "      3        4.3410  0.0196\n",
            "      4        \u001b[36m3.2595\u001b[0m  0.0174\n",
            "      5        \u001b[36m2.1043\u001b[0m  0.0270\n",
            "      6        \u001b[36m1.5524\u001b[0m  0.0229\n",
            "      7        1.7133  0.0205\n",
            "      8        \u001b[36m1.1966\u001b[0m  0.0171\n",
            "      9        1.4066  0.0189\n",
            "     10        \u001b[36m0.9826\u001b[0m  0.0182\n",
            "     11        0.9946  0.0178\n",
            "     12        \u001b[36m0.8881\u001b[0m  0.0176\n",
            "     13        \u001b[36m0.8214\u001b[0m  0.0191\n",
            "     14        0.8225  0.0173\n",
            "     15        \u001b[36m0.7402\u001b[0m  0.0180\n",
            "     16        0.7512  0.0195\n",
            "     17        \u001b[36m0.7377\u001b[0m  0.0192\n",
            "     18        \u001b[36m0.7016\u001b[0m  0.0212\n",
            "     19        0.7154  0.0180\n",
            "     20        \u001b[36m0.6863\u001b[0m  0.0184\n",
            "     21        0.7046  0.0178\n",
            "     22        0.6958  0.0197\n",
            "     23        0.7150  0.0165\n",
            "     24        \u001b[36m0.6451\u001b[0m  0.0187\n",
            "     25        0.6653  0.0192\n",
            "     26        \u001b[36m0.6322\u001b[0m  0.0178\n",
            "     27        0.6873  0.0176\n",
            "     28        \u001b[36m0.5987\u001b[0m  0.0175\n",
            "     29        0.6362  0.0178\n",
            "     30        \u001b[36m0.5560\u001b[0m  0.0166\n",
            "     31        0.5993  0.0238\n",
            "     32        \u001b[36m0.5493\u001b[0m  0.0200\n",
            "     33        0.5969  0.0242\n",
            "     34        \u001b[36m0.5307\u001b[0m  0.0181\n",
            "     35        \u001b[36m0.5232\u001b[0m  0.0186\n",
            "     36        0.5583  0.0185\n",
            "     37        0.5971  0.0184\n",
            "     38        0.5815  0.0205\n",
            "     39        0.5494  0.0180\n",
            "     40        \u001b[36m0.5111\u001b[0m  0.0191\n",
            "     41        0.5290  0.0182\n",
            "     42        0.5333  0.0183\n",
            "     43        0.5146  0.0196\n",
            "     44        \u001b[36m0.4720\u001b[0m  0.0193\n",
            "     45        0.5264  0.0174\n",
            "     46        0.5537  0.0189\n",
            "     47        0.4792  0.0179\n",
            "     48        0.5159  0.0179\n",
            "     49        0.5309  0.0216\n",
            "     50        0.5094  0.0185\n",
            "     51        0.4905  0.0178\n",
            "     52        0.4972  0.0174\n",
            "     53        \u001b[36m0.4417\u001b[0m  0.0183\n",
            "     54        0.4648  0.0176\n",
            "     55        0.4843  0.0176\n",
            "     56        0.4858  0.0176\n",
            "     57        0.4430  0.0168\n",
            "     58        0.4810  0.0177\n",
            "     59        0.4610  0.0170\n",
            "     60        0.4605  0.0177\n",
            "     61        \u001b[36m0.4134\u001b[0m  0.0170\n",
            "     62        0.5063  0.0187\n",
            "     63        0.4413  0.0205\n",
            "     64        \u001b[36m0.3859\u001b[0m  0.0174\n",
            "     65        0.3980  0.0207\n",
            "     66        0.4266  0.0170\n",
            "     67        0.4067  0.0172\n",
            "     68        0.3973  0.0172\n",
            "     69        0.4776  0.0200\n",
            "     70        0.4053  0.0188\n",
            "     71        \u001b[36m0.3745\u001b[0m  0.0198\n",
            "     72        0.4100  0.0175\n",
            "     73        0.4047  0.0182\n",
            "     74        0.4153  0.0184\n",
            "     75        \u001b[36m0.3726\u001b[0m  0.0180\n",
            "     76        0.4111  0.0193\n",
            "     77        0.3870  0.0186\n",
            "     78        \u001b[36m0.3341\u001b[0m  0.0178\n",
            "     79        0.4015  0.0188\n",
            "     80        0.3764  0.0178\n",
            "     81        0.3862  0.0183\n",
            "     82        0.3715  0.0176\n",
            "     83        \u001b[36m0.3284\u001b[0m  0.0191\n",
            "     84        0.3758  0.0257\n",
            "     85        0.3599  0.0207\n",
            "     86        0.3287  0.0184\n",
            "     87        \u001b[36m0.3071\u001b[0m  0.0182\n",
            "     88        0.3483  0.0187\n",
            "     89        \u001b[36m0.2846\u001b[0m  0.0184\n",
            "     90        0.3684  0.0179\n",
            "     91        0.2870  0.0174\n",
            "     92        0.2922  0.0184\n",
            "     93        0.3358  0.0173\n",
            "     94        \u001b[36m0.2797\u001b[0m  0.0195\n",
            "     95        0.3122  0.0185\n",
            "     96        0.3644  0.0167\n",
            "     97        \u001b[36m0.2664\u001b[0m  0.0176\n",
            "     98        0.3062  0.0169\n",
            "     99        0.3314  0.0192\n",
            "    100        0.3415  0.0191\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m6.4722\u001b[0m  0.0147\n",
            "      2        \u001b[36m2.6034\u001b[0m  0.0170\n",
            "      3        \u001b[36m2.3998\u001b[0m  0.0169\n",
            "      4        \u001b[36m1.7377\u001b[0m  0.0169\n",
            "      5        \u001b[36m1.4649\u001b[0m  0.0164\n",
            "      6        \u001b[36m1.1551\u001b[0m  0.0175\n",
            "      7        \u001b[36m1.1515\u001b[0m  0.0170\n",
            "      8        \u001b[36m1.1205\u001b[0m  0.0168\n",
            "      9        \u001b[36m1.1091\u001b[0m  0.0191\n",
            "     10        \u001b[36m1.0828\u001b[0m  0.0173\n",
            "     11        \u001b[36m1.0384\u001b[0m  0.0180\n",
            "     12        1.1392  0.0189\n",
            "     13        \u001b[36m1.0304\u001b[0m  0.0165\n",
            "     14        1.0452  0.0179\n",
            "     15        1.0353  0.0167\n",
            "     16        \u001b[36m0.9572\u001b[0m  0.0170\n",
            "     17        0.9872  0.0179\n",
            "     18        0.9849  0.0173\n",
            "     19        0.9871  0.0161\n",
            "     20        \u001b[36m0.9515\u001b[0m  0.0184\n",
            "     21        \u001b[36m0.9184\u001b[0m  0.0166\n",
            "     22        0.9321  0.0171\n",
            "     23        \u001b[36m0.9002\u001b[0m  0.0162\n",
            "     24        \u001b[36m0.8912\u001b[0m  0.0171\n",
            "     25        \u001b[36m0.8659\u001b[0m  0.0192\n",
            "     26        0.9520  0.0173\n",
            "     27        0.9135  0.0196\n",
            "     28        0.8904  0.0169\n",
            "     29        \u001b[36m0.8573\u001b[0m  0.0194\n",
            "     30        \u001b[36m0.8520\u001b[0m  0.0157\n",
            "     31        \u001b[36m0.8011\u001b[0m  0.0174\n",
            "     32        \u001b[36m0.7739\u001b[0m  0.0169\n",
            "     33        0.8327  0.0180\n",
            "     34        0.7979  0.0166\n",
            "     35        \u001b[36m0.7635\u001b[0m  0.0183\n",
            "     36        0.8650  0.0193\n",
            "     37        \u001b[36m0.7615\u001b[0m  0.0261\n",
            "     38        \u001b[36m0.7445\u001b[0m  0.0190\n",
            "     39        0.7792  0.0160\n",
            "     40        0.7704  0.0157\n",
            "     41        \u001b[36m0.7299\u001b[0m  0.0169\n",
            "     42        0.7417  0.0163\n",
            "     43        \u001b[36m0.7128\u001b[0m  0.0162\n",
            "     44        \u001b[36m0.6923\u001b[0m  0.0173\n",
            "     45        0.7173  0.0159\n",
            "     46        0.7001  0.0160\n",
            "     47        \u001b[36m0.6781\u001b[0m  0.0175\n",
            "     48        0.7020  0.0173\n",
            "     49        0.6839  0.0184\n",
            "     50        0.6927  0.0168\n",
            "     51        0.7003  0.0167\n",
            "     52        0.6911  0.0172\n",
            "     53        0.7242  0.0188\n",
            "     54        0.6792  0.0165\n",
            "     55        0.6800  0.0190\n",
            "     56        0.6821  0.0200\n",
            "     57        \u001b[36m0.6403\u001b[0m  0.0163\n",
            "     58        0.6559  0.0165\n",
            "     59        0.6633  0.0190\n",
            "     60        0.6557  0.0170\n",
            "     61        0.6466  0.0159\n",
            "     62        \u001b[36m0.6384\u001b[0m  0.0188\n",
            "     63        \u001b[36m0.6322\u001b[0m  0.0165\n",
            "     64        0.6412  0.0161\n",
            "     65        0.6627  0.0156\n",
            "     66        0.6535  0.0174\n",
            "     67        \u001b[36m0.6119\u001b[0m  0.0151\n",
            "     68        \u001b[36m0.5923\u001b[0m  0.0169\n",
            "     69        \u001b[36m0.5496\u001b[0m  0.0166\n",
            "     70        0.6886  0.0162\n",
            "     71        0.6420  0.0177\n",
            "     72        0.5885  0.0168\n",
            "     73        0.6401  0.0193\n",
            "     74        0.6442  0.0160\n",
            "     75        0.5627  0.0174\n",
            "     76        0.5615  0.0174\n",
            "     77        0.7008  0.0188\n",
            "     78        0.5818  0.0162\n",
            "     79        0.5632  0.0181\n",
            "     80        0.6353  0.0198\n",
            "     81        0.5662  0.0195\n",
            "     82        0.5914  0.0152\n",
            "     83        0.5682  0.0177\n",
            "     84        0.5968  0.0166\n",
            "     85        0.6040  0.0181\n",
            "     86        0.5764  0.0167\n",
            "     87        0.6407  0.0198\n",
            "     88        0.5856  0.0172\n",
            "     89        0.6464  0.0174\n",
            "     90        0.5787  0.0167\n",
            "     91        0.6009  0.0218\n",
            "     92        0.6133  0.0265\n",
            "     93        0.5736  0.0166\n",
            "     94        0.6958  0.0159\n",
            "     95        0.6327  0.0174\n",
            "     96        \u001b[36m0.5435\u001b[0m  0.0177\n",
            "     97        0.6011  0.0177\n",
            "     98        0.5575  0.0172\n",
            "     99        0.5995  0.0163\n",
            "    100        0.5636  0.0150\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m13.8169\u001b[0m  0.0160\n",
            "      2        \u001b[36m7.0848\u001b[0m  0.0179\n",
            "      3        \u001b[36m3.5270\u001b[0m  0.0155\n",
            "      4        \u001b[36m2.1479\u001b[0m  0.0162\n",
            "      5        \u001b[36m1.7146\u001b[0m  0.0160\n",
            "      6        \u001b[36m1.5125\u001b[0m  0.0187\n",
            "      7        \u001b[36m1.3896\u001b[0m  0.0159\n",
            "      8        1.4412  0.0162\n",
            "      9        \u001b[36m1.2665\u001b[0m  0.0160\n",
            "     10        1.2752  0.0180\n",
            "     11        \u001b[36m1.1648\u001b[0m  0.0173\n",
            "     12        1.2682  0.0178\n",
            "     13        1.2389  0.0171\n",
            "     14        \u001b[36m1.1101\u001b[0m  0.0168\n",
            "     15        1.1755  0.0160\n",
            "     16        1.1955  0.0188\n",
            "     17        1.1653  0.0167\n",
            "     18        1.1161  0.0173\n",
            "     19        1.1626  0.0170\n",
            "     20        1.1571  0.0209\n",
            "     21        1.1638  0.0176\n",
            "     22        1.1110  0.0173\n",
            "     23        1.1128  0.0159\n",
            "     24        \u001b[36m1.1075\u001b[0m  0.0176\n",
            "     25        \u001b[36m1.0909\u001b[0m  0.0221\n",
            "     26        \u001b[36m1.0699\u001b[0m  0.0202\n",
            "     27        \u001b[36m1.0563\u001b[0m  0.0214\n",
            "     28        1.0857  0.0223\n",
            "     29        \u001b[36m1.0417\u001b[0m  0.0225\n",
            "     30        \u001b[36m1.0081\u001b[0m  0.0227\n",
            "     31        1.0301  0.0194\n",
            "     32        1.0437  0.0204\n",
            "     33        1.0101  0.0232\n",
            "     34        1.0333  0.0206\n",
            "     35        1.0132  0.0207\n",
            "     36        1.0207  0.0258\n",
            "     37        1.0124  0.0276\n",
            "     38        \u001b[36m0.9954\u001b[0m  0.0203\n",
            "     39        1.0134  0.0201\n",
            "     40        0.9997  0.0255\n",
            "     41        \u001b[36m0.9627\u001b[0m  0.0353\n",
            "     42        0.9952  0.0214\n",
            "     43        \u001b[36m0.9588\u001b[0m  0.0236\n",
            "     44        0.9637  0.0214\n",
            "     45        0.9827  0.0250\n",
            "     46        \u001b[36m0.9269\u001b[0m  0.0221\n",
            "     47        0.9339  0.0203\n",
            "     48        \u001b[36m0.9076\u001b[0m  0.0197\n",
            "     49        0.9718  0.0216\n",
            "     50        0.9271  0.0229\n",
            "     51        0.9664  0.0203\n",
            "     52        0.9625  0.0216\n",
            "     53        0.9135  0.0203\n",
            "     54        \u001b[36m0.8686\u001b[0m  0.0197\n",
            "     55        0.9307  0.0193\n",
            "     56        0.9009  0.0200\n",
            "     57        0.8790  0.0222\n",
            "     58        0.9389  0.0195\n",
            "     59        0.8976  0.0219\n",
            "     60        0.9389  0.0198\n",
            "     61        \u001b[36m0.8604\u001b[0m  0.0208\n",
            "     62        0.8905  0.0225\n",
            "     63        0.8853  0.0212\n",
            "     64        0.8810  0.0195\n",
            "     65        0.9103  0.0194\n",
            "     66        \u001b[36m0.8534\u001b[0m  0.0188\n",
            "     67        \u001b[36m0.8202\u001b[0m  0.0191\n",
            "     68        0.8603  0.0198\n",
            "     69        0.8375  0.0203\n",
            "     70        0.8261  0.0202\n",
            "     71        0.8443  0.0203\n",
            "     72        0.8959  0.0224\n",
            "     73        0.8823  0.0202\n",
            "     74        0.8226  0.0184\n",
            "     75        0.8587  0.0180\n",
            "     76        0.8318  0.0184\n",
            "     77        0.8513  0.0218\n",
            "     78        0.8564  0.0197\n",
            "     79        0.8523  0.0198\n",
            "     80        0.8331  0.0203\n",
            "     81        \u001b[36m0.8111\u001b[0m  0.0196\n",
            "     82        \u001b[36m0.7972\u001b[0m  0.0201\n",
            "     83        \u001b[36m0.7969\u001b[0m  0.0204\n",
            "     84        0.8267  0.0235\n",
            "     85        0.7994  0.0257\n",
            "     86        \u001b[36m0.7275\u001b[0m  0.0202\n",
            "     87        0.7486  0.0225\n",
            "     88        0.7748  0.0221\n",
            "     89        0.7808  0.0207\n",
            "     90        0.7480  0.0206\n",
            "     91        0.7529  0.0200\n",
            "     92        0.7495  0.0239\n",
            "     93        \u001b[36m0.7050\u001b[0m  0.0232\n",
            "     94        0.7441  0.0238\n",
            "     95        0.7421  0.0231\n",
            "     96        0.7317  0.0263\n",
            "     97        0.7553  0.0229\n",
            "     98        0.7652  0.0225\n",
            "     99        0.7541  0.0252\n",
            "    100        0.7552  0.0250\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m11.7028\u001b[0m  0.0191\n",
            "      2        \u001b[36m5.1582\u001b[0m  0.0252\n",
            "      3        \u001b[36m3.5473\u001b[0m  0.0203\n",
            "      4        \u001b[36m2.2005\u001b[0m  0.0205\n",
            "      5        \u001b[36m1.5189\u001b[0m  0.0196\n",
            "      6        \u001b[36m1.3920\u001b[0m  0.0211\n",
            "      7        \u001b[36m1.2980\u001b[0m  0.0258\n",
            "      8        \u001b[36m1.2367\u001b[0m  0.0200\n",
            "      9        \u001b[36m1.1103\u001b[0m  0.0211\n",
            "     10        1.1558  0.0278\n",
            "     11        1.1676  0.0267\n",
            "     12        \u001b[36m1.1097\u001b[0m  0.0248\n",
            "     13        1.1925  0.0225\n",
            "     14        1.1884  0.0225\n",
            "     15        1.1548  0.0251\n",
            "     16        1.1200  0.0232\n",
            "     17        \u001b[36m1.0814\u001b[0m  0.0234\n",
            "     18        \u001b[36m1.0781\u001b[0m  0.0242\n",
            "     19        1.1103  0.0261\n",
            "     20        \u001b[36m1.0481\u001b[0m  0.0267\n",
            "     21        1.0940  0.0255\n",
            "     22        1.0869  0.0224\n",
            "     23        \u001b[36m1.0315\u001b[0m  0.0243\n",
            "     24        1.0632  0.0337\n",
            "     25        \u001b[36m1.0284\u001b[0m  0.0306\n",
            "     26        1.0817  0.0289\n",
            "     27        1.0648  0.0251\n",
            "     28        1.0638  0.0244\n",
            "     29        1.0598  0.0178\n",
            "     30        1.0533  0.0176\n",
            "     31        1.0339  0.0174\n",
            "     32        1.0731  0.0185\n",
            "     33        \u001b[36m0.9870\u001b[0m  0.0186\n",
            "     34        \u001b[36m0.9556\u001b[0m  0.0185\n",
            "     35        0.9844  0.0163\n",
            "     36        0.9585  0.0165\n",
            "     37        0.9601  0.0166\n",
            "     38        1.0087  0.0160\n",
            "     39        1.0213  0.0180\n",
            "     40        \u001b[36m0.9315\u001b[0m  0.0157\n",
            "     41        0.9511  0.0168\n",
            "     42        \u001b[36m0.8988\u001b[0m  0.0202\n",
            "     43        0.9454  0.0165\n",
            "     44        0.9241  0.0192\n",
            "     45        \u001b[36m0.8938\u001b[0m  0.0169\n",
            "     46        0.9400  0.0156\n",
            "     47        0.8982  0.0194\n",
            "     48        \u001b[36m0.8630\u001b[0m  0.0164\n",
            "     49        0.9311  0.0173\n",
            "     50        0.8790  0.0177\n",
            "     51        0.9041  0.0174\n",
            "     52        \u001b[36m0.8619\u001b[0m  0.0176\n",
            "     53        \u001b[36m0.8117\u001b[0m  0.0186\n",
            "     54        0.8252  0.0193\n",
            "     55        0.8455  0.0183\n",
            "     56        0.8323  0.0170\n",
            "     57        0.8140  0.0166\n",
            "     58        0.8511  0.0184\n",
            "     59        \u001b[36m0.8043\u001b[0m  0.0185\n",
            "     60        0.8288  0.0179\n",
            "     61        0.8130  0.0180\n",
            "     62        0.8389  0.0198\n",
            "     63        0.8093  0.0171\n",
            "     64        0.8326  0.0182\n",
            "     65        \u001b[36m0.8003\u001b[0m  0.0178\n",
            "     66        \u001b[36m0.7934\u001b[0m  0.0174\n",
            "     67        0.8302  0.0184\n",
            "     68        0.8218  0.0167\n",
            "     69        0.8062  0.0182\n",
            "     70        0.7947  0.0184\n",
            "     71        0.8080  0.0164\n",
            "     72        \u001b[36m0.7470\u001b[0m  0.0178\n",
            "     73        \u001b[36m0.7417\u001b[0m  0.0175\n",
            "     74        0.7655  0.0165\n",
            "     75        0.7428  0.0186\n",
            "     76        0.7906  0.0214\n",
            "     77        0.7752  0.0151\n",
            "     78        \u001b[36m0.7263\u001b[0m  0.0207\n",
            "     79        \u001b[36m0.6589\u001b[0m  0.0168\n",
            "     80        0.6668  0.0180\n",
            "     81        0.7877  0.0165\n",
            "     82        0.6776  0.0177\n",
            "     83        0.7571  0.0169\n",
            "     84        0.7307  0.0173\n",
            "     85        0.6869  0.0174\n",
            "     86        0.6743  0.0196\n",
            "     87        0.7853  0.0191\n",
            "     88        0.7255  0.0184\n",
            "     89        0.7247  0.0183\n",
            "     90        \u001b[36m0.6291\u001b[0m  0.0168\n",
            "     91        0.6610  0.0192\n",
            "     92        0.6349  0.0169\n",
            "     93        0.6876  0.0171\n",
            "     94        0.6396  0.0174\n",
            "     95        0.7805  0.0184\n",
            "     96        0.6373  0.0186\n",
            "     97        0.6296  0.0183\n",
            "     98        0.6741  0.0159\n",
            "     99        0.6466  0.0183\n",
            "    100        0.7110  0.0174\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m12.7279\u001b[0m  0.0159\n",
            "      2        \u001b[36m5.3008\u001b[0m  0.0175\n",
            "      3        \u001b[36m3.1495\u001b[0m  0.0169\n",
            "      4        3.3855  0.0195\n",
            "      5        \u001b[36m2.2466\u001b[0m  0.0198\n",
            "      6        \u001b[36m1.5501\u001b[0m  0.0176\n",
            "      7        \u001b[36m1.3655\u001b[0m  0.0191\n",
            "      8        \u001b[36m1.1559\u001b[0m  0.0190\n",
            "      9        \u001b[36m1.0523\u001b[0m  0.0170\n",
            "     10        \u001b[36m0.9783\u001b[0m  0.0191\n",
            "     11        \u001b[36m0.9529\u001b[0m  0.0185\n",
            "     12        0.9616  0.0181\n",
            "     13        \u001b[36m0.9344\u001b[0m  0.0186\n",
            "     14        \u001b[36m0.9156\u001b[0m  0.0185\n",
            "     15        \u001b[36m0.8612\u001b[0m  0.0189\n",
            "     16        0.8854  0.0186\n",
            "     17        \u001b[36m0.8012\u001b[0m  0.0179\n",
            "     18        0.8627  0.0178\n",
            "     19        \u001b[36m0.7876\u001b[0m  0.0182\n",
            "     20        0.8003  0.0186\n",
            "     21        0.7880  0.0172\n",
            "     22        \u001b[36m0.7069\u001b[0m  0.0179\n",
            "     23        0.7270  0.0219\n",
            "     24        0.7274  0.0175\n",
            "     25        0.7096  0.0186\n",
            "     26        \u001b[36m0.6850\u001b[0m  0.0173\n",
            "     27        0.7436  0.0217\n",
            "     28        \u001b[36m0.6635\u001b[0m  0.0191\n",
            "     29        0.6984  0.0171\n",
            "     30        0.6874  0.0179\n",
            "     31        0.6949  0.0167\n",
            "     32        0.6676  0.0169\n",
            "     33        0.6759  0.0163\n",
            "     34        \u001b[36m0.6132\u001b[0m  0.0203\n",
            "     35        0.6292  0.0178\n",
            "     36        0.6743  0.0167\n",
            "     37        0.6295  0.0179\n",
            "     38        0.6258  0.0169\n",
            "     39        \u001b[36m0.6104\u001b[0m  0.0198\n",
            "     40        0.6456  0.0185\n",
            "     41        \u001b[36m0.5916\u001b[0m  0.0196\n",
            "     42        0.5937  0.0191\n",
            "     43        0.6104  0.0184\n",
            "     44        \u001b[36m0.5413\u001b[0m  0.0184\n",
            "     45        0.5983  0.0179\n",
            "     46        0.5594  0.0202\n",
            "     47        0.5594  0.0158\n",
            "     48        0.6265  0.0174\n",
            "     49        \u001b[36m0.5183\u001b[0m  0.0153\n",
            "     50        0.5490  0.0191\n",
            "     51        0.5313  0.0197\n",
            "     52        0.5562  0.0176\n",
            "     53        0.5495  0.0184\n",
            "     54        0.5739  0.0202\n",
            "     55        0.5471  0.0176\n",
            "     56        \u001b[36m0.5110\u001b[0m  0.0194\n",
            "     57        0.5156  0.0172\n",
            "     58        \u001b[36m0.5109\u001b[0m  0.0193\n",
            "     59        0.5333  0.0183\n",
            "     60        \u001b[36m0.5027\u001b[0m  0.0180\n",
            "     61        0.5181  0.0182\n",
            "     62        0.5462  0.0197\n",
            "     63        0.5245  0.0179\n",
            "     64        0.5118  0.0167\n",
            "     65        \u001b[36m0.4686\u001b[0m  0.0184\n",
            "     66        0.4987  0.0174\n",
            "     67        0.5375  0.0209\n",
            "     68        \u001b[36m0.4497\u001b[0m  0.0173\n",
            "     69        0.4714  0.0176\n",
            "     70        0.4872  0.0186\n",
            "     71        0.4762  0.0187\n",
            "     72        0.5015  0.0173\n",
            "     73        0.4524  0.0174\n",
            "     74        0.5327  0.0188\n",
            "     75        0.4561  0.0169\n",
            "     76        \u001b[36m0.4419\u001b[0m  0.0173\n",
            "     77        0.5260  0.0169\n",
            "     78        \u001b[36m0.4195\u001b[0m  0.0192\n",
            "     79        0.4861  0.0182\n",
            "     80        0.5331  0.0220\n",
            "     81        \u001b[36m0.4058\u001b[0m  0.0168\n",
            "     82        0.5046  0.0164\n",
            "     83        0.4213  0.0203\n",
            "     84        0.4257  0.0162\n",
            "     85        0.4375  0.0171\n",
            "     86        0.4284  0.0167\n",
            "     87        0.5127  0.0186\n",
            "     88        0.4593  0.0170\n",
            "     89        \u001b[36m0.4026\u001b[0m  0.0179\n",
            "     90        \u001b[36m0.3989\u001b[0m  0.0183\n",
            "     91        \u001b[36m0.3715\u001b[0m  0.0206\n",
            "     92        0.4469  0.0179\n",
            "     93        0.4316  0.0175\n",
            "     94        0.4833  0.0212\n",
            "     95        0.3971  0.0178\n",
            "     96        0.5123  0.0221\n",
            "     97        0.4021  0.0181\n",
            "     98        0.4339  0.0199\n",
            "     99        0.4015  0.0198\n",
            "    100        0.4479  0.0187\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m17.0126\u001b[0m  0.0179\n",
            "      2        \u001b[36m5.9414\u001b[0m  0.0225\n",
            "      3        \u001b[36m4.3812\u001b[0m  0.0190\n",
            "      4        4.4313  0.0186\n",
            "      5        \u001b[36m2.7401\u001b[0m  0.0172\n",
            "      6        \u001b[36m2.0095\u001b[0m  0.0192\n",
            "      7        \u001b[36m1.7794\u001b[0m  0.0184\n",
            "      8        \u001b[36m1.6894\u001b[0m  0.0180\n",
            "      9        \u001b[36m1.2848\u001b[0m  0.0173\n",
            "     10        1.3097  0.0194\n",
            "     11        \u001b[36m1.2664\u001b[0m  0.0180\n",
            "     12        \u001b[36m1.2215\u001b[0m  0.0177\n",
            "     13        \u001b[36m1.0698\u001b[0m  0.0185\n",
            "     14        1.0759  0.0183\n",
            "     15        \u001b[36m1.0220\u001b[0m  0.0204\n",
            "     16        1.0333  0.0180\n",
            "     17        1.0328  0.0198\n",
            "     18        \u001b[36m1.0218\u001b[0m  0.0188\n",
            "     19        \u001b[36m0.9320\u001b[0m  0.0183\n",
            "     20        0.9871  0.0211\n",
            "     21        0.9708  0.0180\n",
            "     22        0.9430  0.0188\n",
            "     23        0.9393  0.0169\n",
            "     24        \u001b[36m0.8798\u001b[0m  0.0215\n",
            "     25        \u001b[36m0.8730\u001b[0m  0.0180\n",
            "     26        0.8801  0.0178\n",
            "     27        \u001b[36m0.8583\u001b[0m  0.0165\n",
            "     28        0.8865  0.0189\n",
            "     29        \u001b[36m0.8530\u001b[0m  0.0183\n",
            "     30        0.8731  0.0305\n",
            "     31        \u001b[36m0.7859\u001b[0m  0.0244\n",
            "     32        0.8055  0.0189\n",
            "     33        0.8760  0.0163\n",
            "     34        0.8079  0.0173\n",
            "     35        0.8167  0.0172\n",
            "     36        0.8357  0.0210\n",
            "     37        \u001b[36m0.7823\u001b[0m  0.0172\n",
            "     38        0.7998  0.0173\n",
            "     39        0.7948  0.0199\n",
            "     40        \u001b[36m0.7662\u001b[0m  0.0171\n",
            "     41        \u001b[36m0.6984\u001b[0m  0.0167\n",
            "     42        0.7935  0.0240\n",
            "     43        0.8107  0.0172\n",
            "     44        0.7358  0.0178\n",
            "     45        0.7082  0.0173\n",
            "     46        0.8216  0.0189\n",
            "     47        0.7024  0.0181\n",
            "     48        0.7409  0.0186\n",
            "     49        0.7058  0.0179\n",
            "     50        0.7312  0.0196\n",
            "     51        \u001b[36m0.6721\u001b[0m  0.0185\n",
            "     52        \u001b[36m0.6714\u001b[0m  0.0177\n",
            "     53        0.6928  0.0193\n",
            "     54        0.6724  0.0168\n",
            "     55        0.6755  0.0204\n",
            "     56        0.6853  0.0172\n",
            "     57        0.6911  0.0208\n",
            "     58        0.7184  0.0172\n",
            "     59        \u001b[36m0.6414\u001b[0m  0.0195\n",
            "     60        0.6937  0.0191\n",
            "     61        \u001b[36m0.6395\u001b[0m  0.0196\n",
            "     62        0.6763  0.0178\n",
            "     63        0.6618  0.0180\n",
            "     64        0.6680  0.0181\n",
            "     65        \u001b[36m0.6297\u001b[0m  0.0192\n",
            "     66        0.6305  0.0184\n",
            "     67        0.6574  0.0187\n",
            "     68        0.6644  0.0175\n",
            "     69        \u001b[36m0.6264\u001b[0m  0.0170\n",
            "     70        \u001b[36m0.6032\u001b[0m  0.0195\n",
            "     71        0.6173  0.0187\n",
            "     72        0.6590  0.0183\n",
            "     73        0.6282  0.0210\n",
            "     74        0.6272  0.0172\n",
            "     75        0.6119  0.0191\n",
            "     76        \u001b[36m0.5391\u001b[0m  0.0185\n",
            "     77        0.5770  0.0178\n",
            "     78        0.6178  0.0166\n",
            "     79        \u001b[36m0.5273\u001b[0m  0.0203\n",
            "     80        0.6029  0.0176\n",
            "     81        0.5795  0.0170\n",
            "     82        \u001b[36m0.5137\u001b[0m  0.0192\n",
            "     83        0.5689  0.0167\n",
            "     84        0.5209  0.0198\n",
            "     85        0.5636  0.0285\n",
            "     86        0.5527  0.0201\n",
            "     87        0.5469  0.0167\n",
            "     88        0.5372  0.0184\n",
            "     89        0.5546  0.0180\n",
            "     90        0.5329  0.0176\n",
            "     91        0.5556  0.0175\n",
            "     92        0.5308  0.0187\n",
            "     93        0.5602  0.0175\n",
            "     94        \u001b[36m0.5109\u001b[0m  0.0186\n",
            "     95        \u001b[36m0.4642\u001b[0m  0.0167\n",
            "     96        0.6344  0.0182\n",
            "     97        0.5675  0.0177\n",
            "     98        0.5143  0.0168\n",
            "     99        0.5514  0.0171\n",
            "    100        0.5410  0.0195\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m16.9291\u001b[0m  0.0157\n",
            "      2        \u001b[36m8.5122\u001b[0m  0.0199\n",
            "      3        \u001b[36m6.5020\u001b[0m  0.0198\n",
            "      4        \u001b[36m4.5051\u001b[0m  0.0190\n",
            "      5        \u001b[36m3.6845\u001b[0m  0.0157\n",
            "      6        \u001b[36m2.3031\u001b[0m  0.0185\n",
            "      7        \u001b[36m2.0551\u001b[0m  0.0179\n",
            "      8        \u001b[36m1.6572\u001b[0m  0.0181\n",
            "      9        \u001b[36m1.3960\u001b[0m  0.0173\n",
            "     10        \u001b[36m1.3278\u001b[0m  0.0225\n",
            "     11        \u001b[36m1.2727\u001b[0m  0.0180\n",
            "     12        \u001b[36m1.0888\u001b[0m  0.0217\n",
            "     13        1.1736  0.0188\n",
            "     14        1.1643  0.0185\n",
            "     15        \u001b[36m1.0684\u001b[0m  0.0195\n",
            "     16        1.0991  0.0173\n",
            "     17        1.0732  0.0195\n",
            "     18        1.0772  0.0178\n",
            "     19        \u001b[36m1.0230\u001b[0m  0.0181\n",
            "     20        1.0545  0.0171\n",
            "     21        1.0290  0.0195\n",
            "     22        \u001b[36m0.9952\u001b[0m  0.0178\n",
            "     23        1.0011  0.0187\n",
            "     24        \u001b[36m0.9764\u001b[0m  0.0173\n",
            "     25        0.9868  0.0176\n",
            "     26        0.9997  0.0180\n",
            "     27        \u001b[36m0.9520\u001b[0m  0.0193\n",
            "     28        \u001b[36m0.9235\u001b[0m  0.0170\n",
            "     29        \u001b[36m0.8993\u001b[0m  0.0188\n",
            "     30        \u001b[36m0.8550\u001b[0m  0.0182\n",
            "     31        0.8704  0.0178\n",
            "     32        \u001b[36m0.8227\u001b[0m  0.0183\n",
            "     33        0.8686  0.0192\n",
            "     34        \u001b[36m0.7696\u001b[0m  0.0201\n",
            "     35        0.8299  0.0210\n",
            "     36        0.8723  0.0306\n",
            "     37        \u001b[36m0.7552\u001b[0m  0.0198\n",
            "     38        0.7663  0.0197\n",
            "     39        \u001b[36m0.7398\u001b[0m  0.0210\n",
            "     40        0.8765  0.0209\n",
            "     41        0.7589  0.0187\n",
            "     42        0.7730  0.0187\n",
            "     43        \u001b[36m0.7262\u001b[0m  0.0197\n",
            "     44        0.7503  0.0216\n",
            "     45        \u001b[36m0.6966\u001b[0m  0.0165\n",
            "     46        0.7182  0.0168\n",
            "     47        \u001b[36m0.6386\u001b[0m  0.0172\n",
            "     48        0.6929  0.0216\n",
            "     49        0.7021  0.0213\n",
            "     50        0.6548  0.0210\n",
            "     51        0.6420  0.0212\n",
            "     52        \u001b[36m0.6316\u001b[0m  0.0200\n",
            "     53        0.6344  0.0223\n",
            "     54        0.6979  0.0197\n",
            "     55        0.6677  0.0192\n",
            "     56        \u001b[36m0.6114\u001b[0m  0.0175\n",
            "     57        \u001b[36m0.6003\u001b[0m  0.0203\n",
            "     58        0.6301  0.0175\n",
            "     59        \u001b[36m0.5908\u001b[0m  0.0181\n",
            "     60        0.6419  0.0204\n",
            "     61        \u001b[36m0.5681\u001b[0m  0.0193\n",
            "     62        0.6689  0.0215\n",
            "     63        \u001b[36m0.5446\u001b[0m  0.0205\n",
            "     64        0.5832  0.0205\n",
            "     65        0.6024  0.0205\n",
            "     66        0.5667  0.0182\n",
            "     67        0.6241  0.0197\n",
            "     68        0.6401  0.0195\n",
            "     69        0.6111  0.0212\n",
            "     70        \u001b[36m0.5334\u001b[0m  0.0197\n",
            "     71        0.6291  0.0187\n",
            "     72        0.5716  0.0178\n",
            "     73        \u001b[36m0.5247\u001b[0m  0.0179\n",
            "     74        0.6117  0.0171\n",
            "     75        0.5903  0.0207\n",
            "     76        0.6624  0.0182\n",
            "     77        0.5491  0.0173\n",
            "     78        0.5792  0.0166\n",
            "     79        0.6279  0.0186\n",
            "     80        0.5339  0.0180\n",
            "     81        0.5554  0.0185\n",
            "     82        0.6073  0.0162\n",
            "     83        0.5525  0.0177\n",
            "     84        0.5725  0.0177\n",
            "     85        0.5618  0.0210\n",
            "     86        0.6612  0.0178\n",
            "     87        0.5840  0.0204\n",
            "     88        \u001b[36m0.5026\u001b[0m  0.0177\n",
            "     89        0.5866  0.0193\n",
            "     90        0.5427  0.0177\n",
            "     91        0.5850  0.0178\n",
            "     92        0.6096  0.0171\n",
            "     93        0.5921  0.0186\n",
            "     94        0.6043  0.0166\n",
            "     95        0.5667  0.0210\n",
            "     96        0.5349  0.0163\n",
            "     97        0.5589  0.0177\n",
            "     98        0.5431  0.0177\n",
            "     99        0.5796  0.0167\n",
            "    100        \u001b[36m0.4905\u001b[0m  0.0171\n",
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1       \u001b[36m18.7077\u001b[0m  0.0266\n",
            "      2        \u001b[36m5.8754\u001b[0m  0.0237\n",
            "      3        \u001b[36m5.4126\u001b[0m  0.0275\n",
            "      4        \u001b[36m3.4267\u001b[0m  0.0251\n",
            "      5        \u001b[36m2.3758\u001b[0m  0.0261\n",
            "      6        \u001b[36m2.0088\u001b[0m  0.0270\n",
            "      7        \u001b[36m1.6392\u001b[0m  0.0241\n",
            "      8        \u001b[36m1.3983\u001b[0m  0.0238\n",
            "      9        \u001b[36m1.3630\u001b[0m  0.0250\n",
            "     10        \u001b[36m1.2070\u001b[0m  0.0258\n",
            "     11        \u001b[36m0.9485\u001b[0m  0.0254\n",
            "     12        \u001b[36m0.8783\u001b[0m  0.0256\n",
            "     13        0.9207  0.0267\n",
            "     14        \u001b[36m0.8219\u001b[0m  0.0269\n",
            "     15        \u001b[36m0.7805\u001b[0m  0.0252\n",
            "     16        0.8372  0.0246\n",
            "     17        \u001b[36m0.7493\u001b[0m  0.0241\n",
            "     18        \u001b[36m0.7403\u001b[0m  0.0247\n",
            "     19        0.7875  0.0305\n",
            "     20        \u001b[36m0.7400\u001b[0m  0.0252\n",
            "     21        \u001b[36m0.7364\u001b[0m  0.0247\n",
            "     22        \u001b[36m0.6889\u001b[0m  0.0266\n",
            "     23        \u001b[36m0.6881\u001b[0m  0.0271\n",
            "     24        0.7395  0.0276\n",
            "     25        \u001b[36m0.6640\u001b[0m  0.0233\n",
            "     26        0.6820  0.0265\n",
            "     27        0.6856  0.0334\n",
            "     28        \u001b[36m0.6192\u001b[0m  0.0249\n",
            "     29        0.6863  0.0256\n",
            "     30        0.6229  0.0256\n",
            "     31        0.6508  0.0260\n",
            "     32        0.6970  0.0274\n",
            "     33        \u001b[36m0.6012\u001b[0m  0.0260\n",
            "     34        0.6454  0.0244\n",
            "     35        0.6170  0.0273\n",
            "     36        0.6059  0.0253\n",
            "     37        \u001b[36m0.5864\u001b[0m  0.0259\n",
            "     38        \u001b[36m0.5662\u001b[0m  0.0284\n",
            "     39        0.5975  0.0259\n",
            "     40        \u001b[36m0.5584\u001b[0m  0.0273\n",
            "     41        0.5671  0.0253\n",
            "     42        \u001b[36m0.5467\u001b[0m  0.0256\n",
            "     43        0.5476  0.0257\n",
            "     44        0.5507  0.0256\n",
            "     45        \u001b[36m0.5309\u001b[0m  0.0255\n",
            "     46        0.5462  0.0268\n",
            "     47        0.5382  0.0256\n",
            "     48        \u001b[36m0.5275\u001b[0m  0.0282\n",
            "     49        0.5429  0.0246\n",
            "     50        \u001b[36m0.4924\u001b[0m  0.0247\n",
            "     51        0.4950  0.0236\n",
            "     52        0.5311  0.0253\n",
            "     53        0.5190  0.0259\n",
            "     54        0.5148  0.0289\n",
            "     55        \u001b[36m0.4751\u001b[0m  0.0256\n",
            "     56        0.5002  0.0254\n",
            "     57        \u001b[36m0.4637\u001b[0m  0.0288\n",
            "     58        0.5172  0.0271\n",
            "     59        \u001b[36m0.4634\u001b[0m  0.0275\n",
            "     60        \u001b[36m0.4438\u001b[0m  0.0269\n",
            "     61        \u001b[36m0.4292\u001b[0m  0.0247\n",
            "     62        0.4884  0.0254\n",
            "     63        0.4385  0.0246\n",
            "     64        0.4771  0.0309\n",
            "     65        0.4442  0.0244\n",
            "     66        \u001b[36m0.4198\u001b[0m  0.0266\n",
            "     67        \u001b[36m0.4076\u001b[0m  0.0285\n",
            "     68        0.4138  0.0251\n",
            "     69        \u001b[36m0.3965\u001b[0m  0.0259\n",
            "     70        0.4841  0.0253\n",
            "     71        0.4633  0.0308\n",
            "     72        0.4118  0.0267\n",
            "     73        0.4648  0.0279\n",
            "     74        0.4093  0.0307\n",
            "     75        0.5226  0.0276\n",
            "     76        \u001b[36m0.3695\u001b[0m  0.0260\n",
            "     77        0.4528  0.0274\n",
            "     78        \u001b[36m0.3594\u001b[0m  0.0274\n",
            "     79        0.5034  0.0270\n",
            "     80        0.3619  0.0294\n",
            "     81        0.4409  0.0270\n",
            "     82        \u001b[36m0.3038\u001b[0m  0.0286\n",
            "     83        0.4265  0.0271\n",
            "     84        0.3610  0.0269\n",
            "     85        0.4032  0.0254\n",
            "     86        0.4034  0.0281\n",
            "     87        0.4118  0.0260\n",
            "     88        \u001b[36m0.2764\u001b[0m  0.0258\n",
            "     89        0.4019  0.0259\n",
            "     90        0.3386  0.0256\n",
            "     91        0.3772  0.0282\n",
            "     92        0.3058  0.0343\n",
            "     93        0.3611  0.0357\n",
            "     94        0.3274  0.0335\n",
            "     95        0.3375  0.0386\n",
            "     96        0.3250  0.0343\n",
            "     97        0.3674  0.0392\n",
            "     98        0.3351  0.0404\n",
            "     99        0.3049  0.0325\n",
            "    100        0.2955  0.0350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_"
      ],
      "metadata": {
        "id": "LmlEu_RcZnJf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_parameters, best_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUuIjl2eZr-s",
        "outputId": "3e549215-114a-46c5-952f-62290917cd85"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'batch_size': 10,\n",
              "  'max_epochs': 100,\n",
              "  'module__activation': <function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor>,\n",
              "  'module__dropout': 0.2,\n",
              "  'module__initializer': <function torch.nn.init.uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0, generator: Optional[torch._C.Generator] = None) -> torch.Tensor>,\n",
              "  'module__neurons': 8,\n",
              "  'optimizer': torch.optim.adam.Adam},\n",
              " 0.9933333333333333)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "JcBhzcqUaD14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecasters = torch.tensor(forecasters, dtype= torch.float)\n",
        "labels = torch.tensor(labels, dtype=torch.long)"
      ],
      "metadata": {
        "id": "nrm4uMITaDYG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torch.utils.data.TensorDataset(\n",
        "    forecasters,\n",
        "    labels\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=10,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "W_OuOfDOaJzN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = nn.Sequential(\n",
        "    nn.Linear(4,8),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "\n",
        "    nn.Linear(8,8),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.2),\n",
        "\n",
        "    nn.Linear(8,3)\n",
        ")"
      ],
      "metadata": {
        "id": "SMN0sZZTaZ1C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters(),\n",
        "                       lr = 0.001,\n",
        "                       weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "Xr_DJpteaswF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "\n",
        "  running_loss = 0.\n",
        "  running_accuracy = 0.\n",
        "\n",
        "  for data in train_loader:\n",
        "    inputs, label = data\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = classifier(inputs)\n",
        "\n",
        "    loss = criterion(outputs, label)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    ps = F.softmax(outputs)\n",
        "    top_p, top_class = ps.topk(k=1, dim=1)\n",
        "\n",
        "    equals = top_class == label.view(*top_class.shape)\n",
        "    running_accuracy += torch.mean(equals.type(torch.float))\n",
        "\n",
        "  print('Epoch {:3d}: Loss {:3.5f} - Accuracy {:3.5}'.format(epoch +1, running_loss/len(train_loader), running_accuracy/len(train_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqRdquj5bFfL",
        "outputId": "9ed13fad-d1be-4880-cc0a-c6b912e246f2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-67169367f3e1>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  ps = F.softmax(outputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1: Loss 1.18153 - Accuracy 0.34667\n",
            "Epoch   2: Loss 1.14418 - Accuracy 0.36\n",
            "Epoch   3: Loss 1.10241 - Accuracy 0.38\n",
            "Epoch   4: Loss 1.11799 - Accuracy 0.35333\n",
            "Epoch   5: Loss 1.11237 - Accuracy 0.36667\n",
            "Epoch   6: Loss 1.08526 - Accuracy 0.4\n",
            "Epoch   7: Loss 1.06622 - Accuracy 0.38667\n",
            "Epoch   8: Loss 1.09126 - Accuracy 0.38\n",
            "Epoch   9: Loss 1.09909 - Accuracy 0.34\n",
            "Epoch  10: Loss 1.09519 - Accuracy 0.34\n",
            "Epoch  11: Loss 1.02333 - Accuracy 0.45333\n",
            "Epoch  12: Loss 1.04945 - Accuracy 0.39333\n",
            "Epoch  13: Loss 1.02735 - Accuracy 0.38667\n",
            "Epoch  14: Loss 1.01069 - Accuracy 0.45333\n",
            "Epoch  15: Loss 1.02261 - Accuracy 0.41333\n",
            "Epoch  16: Loss 0.98194 - Accuracy 0.47333\n",
            "Epoch  17: Loss 0.99294 - Accuracy 0.47333\n",
            "Epoch  18: Loss 0.96169 - Accuracy 0.52667\n",
            "Epoch  19: Loss 0.92486 - Accuracy 0.52\n",
            "Epoch  20: Loss 0.98922 - Accuracy 0.48667\n",
            "Epoch  21: Loss 0.90212 - Accuracy 0.57333\n",
            "Epoch  22: Loss 0.91698 - Accuracy 0.58667\n",
            "Epoch  23: Loss 0.92512 - Accuracy 0.56667\n",
            "Epoch  24: Loss 0.89319 - Accuracy 0.63333\n",
            "Epoch  25: Loss 0.85748 - Accuracy 0.65333\n",
            "Epoch  26: Loss 0.92079 - Accuracy 0.54667\n",
            "Epoch  27: Loss 0.85261 - Accuracy 0.67333\n",
            "Epoch  28: Loss 0.87877 - Accuracy 0.63333\n",
            "Epoch  29: Loss 0.84215 - Accuracy 0.68\n",
            "Epoch  30: Loss 0.83518 - Accuracy 0.67333\n",
            "Epoch  31: Loss 0.84043 - Accuracy 0.66\n",
            "Epoch  32: Loss 0.85413 - Accuracy 0.64\n",
            "Epoch  33: Loss 0.82561 - Accuracy 0.68\n",
            "Epoch  34: Loss 0.82622 - Accuracy 0.64667\n",
            "Epoch  35: Loss 0.82490 - Accuracy 0.7\n",
            "Epoch  36: Loss 0.76722 - Accuracy 0.75333\n",
            "Epoch  37: Loss 0.79590 - Accuracy 0.68\n",
            "Epoch  38: Loss 0.80053 - Accuracy 0.64\n",
            "Epoch  39: Loss 0.78794 - Accuracy 0.65333\n",
            "Epoch  40: Loss 0.80113 - Accuracy 0.68\n",
            "Epoch  41: Loss 0.77323 - Accuracy 0.66667\n",
            "Epoch  42: Loss 0.77029 - Accuracy 0.69333\n",
            "Epoch  43: Loss 0.75659 - Accuracy 0.71333\n",
            "Epoch  44: Loss 0.79718 - Accuracy 0.62667\n",
            "Epoch  45: Loss 0.77765 - Accuracy 0.67333\n",
            "Epoch  46: Loss 0.80608 - Accuracy 0.65333\n",
            "Epoch  47: Loss 0.77287 - Accuracy 0.73333\n",
            "Epoch  48: Loss 0.70639 - Accuracy 0.72667\n",
            "Epoch  49: Loss 0.71700 - Accuracy 0.69333\n",
            "Epoch  50: Loss 0.69331 - Accuracy 0.75333\n",
            "Epoch  51: Loss 0.71992 - Accuracy 0.71333\n",
            "Epoch  52: Loss 0.72603 - Accuracy 0.68667\n",
            "Epoch  53: Loss 0.68967 - Accuracy 0.68667\n",
            "Epoch  54: Loss 0.69361 - Accuracy 0.71333\n",
            "Epoch  55: Loss 0.69489 - Accuracy 0.64667\n",
            "Epoch  56: Loss 0.66478 - Accuracy 0.68\n",
            "Epoch  57: Loss 0.66165 - Accuracy 0.68\n",
            "Epoch  58: Loss 0.65609 - Accuracy 0.72\n",
            "Epoch  59: Loss 0.67630 - Accuracy 0.72\n",
            "Epoch  60: Loss 0.64269 - Accuracy 0.69333\n",
            "Epoch  61: Loss 0.69042 - Accuracy 0.65333\n",
            "Epoch  62: Loss 0.63961 - Accuracy 0.72\n",
            "Epoch  63: Loss 0.64018 - Accuracy 0.76\n",
            "Epoch  64: Loss 0.65571 - Accuracy 0.68\n",
            "Epoch  65: Loss 0.67609 - Accuracy 0.66667\n",
            "Epoch  66: Loss 0.65726 - Accuracy 0.7\n",
            "Epoch  67: Loss 0.58839 - Accuracy 0.79333\n",
            "Epoch  68: Loss 0.59993 - Accuracy 0.78\n",
            "Epoch  69: Loss 0.62230 - Accuracy 0.68\n",
            "Epoch  70: Loss 0.67632 - Accuracy 0.71333\n",
            "Epoch  71: Loss 0.65097 - Accuracy 0.62667\n",
            "Epoch  72: Loss 0.65233 - Accuracy 0.71333\n",
            "Epoch  73: Loss 0.60545 - Accuracy 0.74\n",
            "Epoch  74: Loss 0.60313 - Accuracy 0.72667\n",
            "Epoch  75: Loss 0.58399 - Accuracy 0.76667\n",
            "Epoch  76: Loss 0.56094 - Accuracy 0.78\n",
            "Epoch  77: Loss 0.60077 - Accuracy 0.73333\n",
            "Epoch  78: Loss 0.62013 - Accuracy 0.70667\n",
            "Epoch  79: Loss 0.59440 - Accuracy 0.72667\n",
            "Epoch  80: Loss 0.53163 - Accuracy 0.78\n",
            "Epoch  81: Loss 0.60275 - Accuracy 0.7\n",
            "Epoch  82: Loss 0.52614 - Accuracy 0.76667\n",
            "Epoch  83: Loss 0.53608 - Accuracy 0.73333\n",
            "Epoch  84: Loss 0.55889 - Accuracy 0.71333\n",
            "Epoch  85: Loss 0.53288 - Accuracy 0.77333\n",
            "Epoch  86: Loss 0.56738 - Accuracy 0.72\n",
            "Epoch  87: Loss 0.59706 - Accuracy 0.76667\n",
            "Epoch  88: Loss 0.53459 - Accuracy 0.79333\n",
            "Epoch  89: Loss 0.48801 - Accuracy 0.76667\n",
            "Epoch  90: Loss 0.51138 - Accuracy 0.8\n",
            "Epoch  91: Loss 0.50562 - Accuracy 0.79333\n",
            "Epoch  92: Loss 0.54852 - Accuracy 0.79333\n",
            "Epoch  93: Loss 0.51178 - Accuracy 0.78667\n",
            "Epoch  94: Loss 0.49021 - Accuracy 0.80667\n",
            "Epoch  95: Loss 0.50671 - Accuracy 0.82\n",
            "Epoch  96: Loss 0.45764 - Accuracy 0.82667\n",
            "Epoch  97: Loss 0.49833 - Accuracy 0.79333\n",
            "Epoch  98: Loss 0.42055 - Accuracy 0.86\n",
            "Epoch  99: Loss 0.45532 - Accuracy 0.83333\n",
            "Epoch 100: Loss 0.47603 - Accuracy 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(classifier.state_dict(), \"iris.pth\")"
      ],
      "metadata": {
        "id": "a5vyO4occMNd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('/content/iris.pth')\n",
        "classifier.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnmryqepcPP7",
        "outputId": "28f1f3b8-e273-4558-b8d8-9455798f0a9d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new = torch.tensor([[3.2, 4.5, 0.9, 1.1]], requires_grad=False)"
      ],
      "metadata": {
        "id": "yd4Gpgy1cV_n"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bl_ZUCLccrO",
        "outputId": "8891f71b-90de-4d9c-b6e2-82b875e67d0a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Dropout(p=0.2, inplace=False)\n",
              "  (3): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (4): ReLU()\n",
              "  (5): Dropout(p=0.2, inplace=False)\n",
              "  (6): Linear(in_features=8, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forecasting = classifier(new)\n",
        "forecasting = F.softmax(forecasting)\n",
        "forecasting = (forecasting > 0.5).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkBI3fQhceRO",
        "outputId": "44cb49b4-2fac-437f-ded2-1d8f6f079f5f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-7e0636ab488f>:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  forecasting = F.softmax(forecasting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if forecasting[0][0] == True and forecasting[0][1] == False and forecasting[0][2] == False:\n",
        "    print('Iris setosa')\n",
        "elif forecasting[0][0] == False and forecasting[0][1] == True and forecasting[0][2] == False:\n",
        "    print('Iris virginica')\n",
        "elif forecasting[0][0] == False and forecasting[0][1] == False and forecasting[0][2] == True:\n",
        "    print('Iris versicolor')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmLUGMUUckux",
        "outputId": "93a52efc-9546-47ee-fb3c-db0439b7ea4d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris setosa\n"
          ]
        }
      ]
    }
  ]
}